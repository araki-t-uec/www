%\section{考察}
これらを踏まえて全体の結果を表~\ref{expetiments_result}に示す．



\begin{table}[tb]
 \centering
 \caption{各実験比較表}\label{expetiments_result}
 \scalebox{0.70}[0.70]{
  \begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c|}
   \hline \hline
   & \rotatebox{90}{bark}& \rotatebox{90}{cling}&\rotatebox{90}{command}& \rotatebox{90}{eat}&\rotatebox{90}{handler}& \rotatebox{90}{run}&\rotatebox{90}{victim}& \rotatebox{90}{shake}& \rotatebox{90}{sniff}& \rotatebox{90}{stop}& \rotatebox{90}{walk} & \rotatebox{90}{全体}\\ \hline
(1) 静止画像   & 0.244& 0.066& 0.0& 0.024& 0.057& 0.0& 0.204& 0.0& 0.0& 0.588& 0.51&  0.436 \\ \hline
(2) optical flow   & 0.141& 0.0& 0.0& 0.0& 0.017& 0.0& 0.017& 0.0& 0.0& 0.586& 0.476&  0.406 \\ \hline
(3) 音声 (Conv1D)   & {\bf 0.669}& 0.078& 0.22& 0.023& 0.138& 0.0& 0.274& {\bf 0.44}& 0.502& 0.745& 0.704&  0.512 \\ \hline
(4) 音声 (Conv2D)   & 0.563& 0.04& 0.188& 0.001& 0.059& 0.0& 0.201& 0.304& 0.524& 0.744& 0.74&  0.512 \\ \hline
(5) 静止画像+optical   & 0.11& 0.018& 0.043& 0.0& 0.155& 0.0& 0.259& 0.0& 0.426& 0.705& 0.668&  0.435 \\ \hline
(6) 静止画像+音声   & 0.662& 0.031& 0.195& 0.018& 0.115& 0.002& 0.308& 0.402& 0.498& 0.726& 0.694&  0.5 \\ \hline
(7) optical flow+音声   & 0.667& 0.054& {\bf 0.234}& 0.014& 0.123& 0.01& 0.223& 0.356& 0.487& 0.759& 0.692&  0.493 \\ \hline
(8) 静止+optical+音声   & 0.577& {\bf 0.135}& 0.186& {\bf 0.066}& {\bf 0.183}& {\bf 0.026}& {\bf 0.433}& 0.409& {\bf 0.53}& {\bf 0.779}& {\bf 0.725}& {\bf 0.518} \\ \hline
  \end{tabular}
 }
\end{table}




\chapter{おわりに}
\section{まとめ}
Sound/image-based three-stream CNNの提案と，提案手法を用いたレスキュー犬の行動推定を行なった．
3つの入力についてアブレーションスタディを行い，提案手法と比較した．
それぞれ単体では音声データがマルチクラス推定に有力であり，その他2つと組み合わせた際に精度が下がる．これから，音声データが重要視するべき情報であると判断できる．
しかし，結果は提案手法であるsound/image-based three-stream CNNが最も精度が高かった．
これらから，音声データはクラス推定に強力であるものの，音声・静止画像・optical flow画像の3つのデータにそれぞれ必要な情報が含まれていることが示された．

精度は51.8\%と数値では決して高いとは言えない．
しかし，これは約30fpsの動画で1/5秒毎に行なった推定の結果である．
犬の行動推定に用いた時間である1秒間で30フレーム毎に推定を行い，さらにその平均を取るなどしてフレーム間の結果を統合する手法などを適用することによりさらなる精度向上が期待できる．

本研究の目的はレスキュー犬の行動推定という人命のかかったタスクである．ハンドラーの補助的な役割を任せた運用をこなせるとしても，実際に現場で判断を任せるにはまだまだ不十分な結果となった．




\section{今後の課題}
本研究では主にレスキュー犬の一人称動画から抽出したデータを用いてレスキュー犬行動推定を行なった．
しかし，optical flow画像の抽出や音声の切り取りなどの簡単な手法を用いた抽出法のみである．
動画からの特徴抽出には工夫の余地が大幅に残されている．3つのデータからのより意味的な特徴の取り出し方を検討するべきである．
映像からの特徴抽出の際には犬一人称画像特有の処理を入れるなどが考えられる．
例えば，人の一人称視点映像の分類手法\cite{minghuang2016fpar}で用いられている腕のセグメンテーションネットワークのように，犬領域を推定するようなネットワークは犬動作の推定にも応用できる．
これは犬の状態推定への貢献が期待される．
ただし，腕領域とは異なり犬領域のデータセットは確認できていない．
%動き情報の出力には，より精度の高いoptical flowを求めるなどが考えられる．
音声からの特徴抽出は，実験に対する疑問として音声フレームの長さが適しているのかという疑問も残っている．今回は検証しなかったが，より推定に最適なフレーム長を求めるべきだ．
より適切な音声フレーム長が判断できればより高い精度も期待できる．
音声の特徴抽出についても最適と断言するには至っていない．今回音声の特徴抽出にmfccを採用したが，\cite{aytar2016soundnet}のように波形をそのまま入力する分類手法も存在する．今回は取り扱わなかったが，検討の価値があるだろう．

レスキュー犬の行動推定の最も重要な課題は精度向上だが，そのための学習データの増強もまた重要な課題の1つである．
例えば領域推定の手法を採用する場合，データセットの用意が必要になる．
レスキュー犬訓練データセットも本研究で利用した内容は十分ではない．特に，eat，shake, runクラスなどは圧倒的にデータ量が少ない．特にeatクラスは，その多様性に対しての数が少なく学習が困難である．
レスキュー犬訓練データの増強は必須課題とも言える．
また，レスキュー犬のクラス分類というタスクとは別に，eat, drinkクラスを分けるなど動画のマルチクラス推定というタスクのために適切なクラス設定での精度向上も考えられる．

さらに，今回は研究の範囲としなかったが，レスキュー犬行動動画の入力に対してリアルタイムに結果を出すことも求められる．

%{\scriptsize % 7pt
%{\footnotesize % 8pt
%{\small % 9pt
%\bibliographystyle{ieee}
\bibliographystyle{junsrt}
\bibliography{ref}
%}
% \begin{footnotesize}
% %{\small
% \bibliography{ref}
% \bibliographystyle{junsrt}
% %}
% \end{footnotesize}

\chapter*{謝辞}
本論文は筆者が電気通信大学大学院情報理工学研究科情報学専攻博士前期課程に在籍中の成果をまとめたものである．
筆者一人では執筆に至らず，多くの人に助けられ本稿の完成に至った．
% 主な協力者を抜粋し以下に謝意を述べる．

特に，
東北大学大学院情報科学研究科准教授・大野和則先生，ならびに同大学未来科学技術研究センター助教・濱田龍之介先生，ならびにデータセット作成に携わったレスキュー犬らによって本研究は成り立っている．
データを集めて体系的に整理して作成されたデータセットの提供を頂いた．ここに同氏らに対して感謝の意を表する．

%指導教員の教授，柳井啓司先生には，分野の異なる学部生を受け入れ論文を書くにまで育てて戴いた．
%不思議な学生の受け入れに慣れた先生で，丁寧な気配りと力強い指導を戴いた．本研究実施の機会を与えて戴いたこととならべ，ここに感謝の意を表する．
%同期の電気通信大学大学院生，會下拓実には，隣の席で常にアドバイスを戴いていた．研究遂行にあたって，助言に加え有益な討論を戴いたことに，ここに感謝の意を表する．

%また，東北公益文科大学公益学部准教授の廣瀬雄二先生ならびに西村まどか先生には，この分野に飛び込むにあたり，その土壌を作り背中を押して戴いた．ここに感謝の意を表する．

%最後に，生活の全てを後押ししてくれた両親ならびに家族に感謝の意を表する．

%書ききれない誰が欠けていても本論文の完成には至らなかった．再度，深く感謝の意を示し結びとする．

\end{document}

