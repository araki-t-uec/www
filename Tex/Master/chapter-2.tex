\chapter{関連研究}
本研究では犬の一人称視点動画からの犬の活動分類を行う．人間のライフログとしての一人称動画の分類や，車載映像からの車の行動推定，第三者視点での動画分類，音声を用いた動画分類などについて紹介し，本研究との関連を述べる．

\section{タフ・ロボティクス・チャレンジ}
政府による総合科学技術・イノベーション会議が研究開発を促進している，``革新的研究開発推進プログラムImPACT''というプログラムがある~\cite{impact}．``ImPACTは研究開発を促進し，持続可能な発展性のあるイノベーションシステムの実現を目指したプログラム''であり，複数の研究開発プログラムを包括している．
タフ・ロボティクス・チャレンジはそのプラグラムのうちの一つであり，遠隔自律ロボット，屋外ロボットサービス事業の実現を目指したプログラムである．
このプログラムでは首都圏直下型地震などを想定し，刻々と変化する厳しい環境下でも実用性を保つ災害救助を目的としたロボットの研究開発が行われている．倒壊家屋や配管内を探索するロボット，悪天候でも飛行するドローンなどを用いての計測や認識，マッピング，活動支援などが達成目標として掲げられる．

\subsection{サイバー救助犬}
サイバー救助犬の研究はタフ・ロボティクス・チャレンジの一つである．災害救助用サイボーグ犬の開発を見据え，その足がかりとして研究されている．
サイバー救助犬の技術的達成目標は``救助犬の行動と状態の計測・伝送・認識・マッピング（運動・映像・声・生体信号）と制御による、救助活動支援''とされており，レスキュー犬の行動をモニタリングするために，濱田，大野らによって装着型計測・記録装置が開発された~\cite{dog01}．
図\ref{cyberdog}にレスキュー犬に装着可能な軽量な行動計測スーツ示す．これを着用したレスキュー犬はサイバー救助犬とも呼ばれる．
サイバー救助犬は各種センサを用いた計測データを記録し，リアルタイムに映像などのデータの無線配信が可能である．そのため，人の目の及ばない範囲でレスキュー犬が活動する際にもレスキュー犬の行動やその周辺環境などが把握可能である．

\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=9cm]{./Figures/cyberdog.eps}
  \caption{装着型計測・記録装置~\cite{dog01}より引用}
  \label{cyberdog}
 \end{center}
\end{figure}

\section{動画認識}
犬一人称視点映像の動きや音声の特徴は，レスキュー犬の周辺環境を知るための重要な手掛かりの1つである．
レスキュー犬の一人称動画に限らず，動画から特徴を取得してその内容を分類する類の研究は行われている．
\subsection{動作認識}
映像から動き特徴を抽出する手法は大きく分けて2つある．
1つはあらかじめ動画を複数枚の画像に分割してから特徴量を抽出する手法である．
もう1つは動画から直接特徴量を抽出する手法である．
前者は既存の画像認識の技術を簡単に流用でき，入力データが比較的小さいので学習コストが低い．
対して後者はフレーム間の情報を考慮できるが，動画を直接入力データとするため学習コストが非常に高い．
\subsubsection{Two-stream}
事前に動画から静止画を切り出してから特徴を抽出し学習する手法としてはSimonyanらによるTwo-stream convolutional networksがある~\cite{simonyan2014two}, \cite{wang2015towards}．
これは，1つの動画から通常のRGB画像とoptical flow画像を抽出し，それぞれを入力とする個々のネットワークを学習することで動き情報を考慮して動画を分類する手法である．
図~\ref{2st_network}にTwo-stream convolutionalのネットワーク構造を示す．
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=12cm]{./Figures/two-stream.eps}
  \caption{Two-stream convolutional networks アーキテクチャ~(\cite{simonyan2014two}より引用)．切り出したRGB画像とoptical flow画像を個々のネットワークに入力し、出力を合わせている．}
  \label{2st_network}
 \end{center}
\end{figure}

\subsubsection{3D Convolution}
動画から直接特徴を抽出して学習する手法としてはTranらによる3D Convolution がある~\cite{tran14}．画像に対して2次元であったフィルターを3次元形状に拡張することで，縦横の空間以外である時間方向への広がりを持って特徴抽出が可能になった．
図~\ref{3dconv_image}に3D Convolutionalの詳細を示す．
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=15cm]{./Figures/3dconv.eps}
  \caption{3D Convolutionの詳細~(\cite{tran14}より引用)．2D Convolutionでは縦横方向の畳み込みを行っており，3D Convolutionでは加えて時間方向への畳み込みを行っている．}
  \label{3dconv_image}
 \end{center}
\end{figure}

\subsubsection{Modeling Dog Behavior From Visual Data}

また，Ehsanらによる犬の一人称視点動画からの犬行動予測の研究がある~\cite{whoretthedog}．これは，犬の行動をモデリングし，犬が次にどのような道をたどり行動するかを予測している．

%\section{問題点}
しかし，これらの研究は犬の行動のモデリングであり，犬の周辺環境の推定などは行っていない．また，入力は動画像のみであり，音声などのデータは利用していない．レスキュー犬の課題には，犬の周辺環境情報や動画像からだけでは判断できない情報の取得が含まれている．例えばレスキュー犬は要救助者を発見するとその場で待機し吠え続けるように訓練されている．このように，動画像データからだけではなく，音声データ，および慣性データ・GPSデータなどの情報を複合的に用いてレスキュー犬の状態を判断しなければならない．本研究は動画像と音声からなるマルチモーダルな情報を入力とした犬の行動の分類を目的としている． 


\subsection{音声分類}
音声と画像から特徴を抽出する研究には以下のようなものがある．
\subsubsection{Sound Net}
音声をクラス分類する研究としてAyterらによるSound Netがある~\cite{aytar2016soundnet}．
動画から音声と画像を取り出し，画像を教師データとし，音声は生徒データとして出力が等しくなるように学習している．
図~\ref{soundnet_network}にSound Netのネットワーク構造を示す．
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=15cm]{./Figures/soundnet.eps}
  \caption{Sound Netのアーキテクチャ~(\cite{aytar2016soundnet}より引用)．動画から映像と音声を切り分け，音声に対して1次元の畳み込みを行なっている．}
  \label{soundnet_network}
 \end{center}
\end{figure}

\subsubsection{Audio-Visual Scene Analysis}
音声と動画を紐づけて，その関係を明らかにする研究としてOwensらによるAudio-Visual Scene Analysisがある~\ref{multisensory2018}．
映像内の音源特定，音声からの動作認識，複数の話者が個々の画面にいる際の話者の特定を行なっており，音声と映像の関連性を示している．
具体的な図を~\ref{audio_visual}に示す．
\begin{figure}[htbp]
 \begin{center}
  \includegraphics[width=15cm]{./Figures/audio_visual.eps}
  \caption{Audio-Visual Scene Analysis~(\cite{multisensory2018}より引用)．}
  \label{audio_visual}
 \end{center}
\end{figure}
